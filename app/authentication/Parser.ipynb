{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\At\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#* Make Dataset *#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.DataFrame()\n",
    "# def load_and_rename_data():\n",
    "#     global merged_df\n",
    "#     data_files = {\n",
    "#         'd1' : './dataset/Accountant.csv',\n",
    "#         'd2' : './dataset/Administration.csv',\n",
    "#         'd3' : './dataset/DataEngineer.csv',\n",
    "#         'd4' : './dataset/DataScientist.csv',\n",
    "#         'd5' : './dataset/HumanResource.csv',\n",
    "#         'd6' : './dataset/MachineLearningEngineer.csv',\n",
    "#         'd7' : './dataset/Resume.csv',\n",
    "#         'd8' : './dataset/UpdatedResumeDataSet.csv'\n",
    "#     }\n",
    "#     data_arr = []\n",
    "\n",
    "#     for path in data_files.values():\n",
    "#         df = pd.read_csv(path)\n",
    "#         print()\n",
    "#         column_mapping = {'Resume_str': 'resume', 'Category': 'category'}\n",
    "#         for old_name in column_mapping:\n",
    "#             if old_name in df.columns:\n",
    "#                 df.rename(columns={'Resume_str': 'resume', 'Resume': 'resume', 'Category': 'category'}, inplace=True)\n",
    "\n",
    "#         if 'resume' in df.columns and 'category' in df.columns:\n",
    "#             df = df[['resume', 'category']]\n",
    "#         else:\n",
    "#             print(f\"Warning: 'resume' and 'category' columns not found in DataFrame {path}\")\n",
    "#         data_arr.append(df)\n",
    "#         merged_df = pd.concat(data_arr, ignore_index=True)\n",
    "#     return merged_df\n",
    "# merged_data = load_and_rename_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>resume</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\nContact\\nwww.linkedin.com/in/mansoor-\\nah...</td>\n",
       "      <td>Accountant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nContact\\nwww.linkedin.com/in/syed-\\nali-a...</td>\n",
       "      <td>Accountant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\nContact\\nwww.linkedin.com/in/babar-\\nkhan...</td>\n",
       "      <td>Accountant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nContact\\nwww.linkedin.com/in/asra-\\nhamid...</td>\n",
       "      <td>Accountant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\nContact\\nwww.linkedin.com/in/umair-azam-\\...</td>\n",
       "      <td>Accountant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             resume    category\n",
       "0           0     \\nContact\\nwww.linkedin.com/in/mansoor-\\nah...  Accountant\n",
       "1           1     \\nContact\\nwww.linkedin.com/in/syed-\\nali-a...  Accountant\n",
       "2           2     \\nContact\\nwww.linkedin.com/in/babar-\\nkhan...  Accountant\n",
       "3           3     \\nContact\\nwww.linkedin.com/in/asra-\\nhamid...  Accountant\n",
       "4           4     \\nContact\\nwww.linkedin.com/in/umair-azam-\\...  Accountant"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../static/dataset/merged_data.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanResume(txt):\n",
    "    cleanText = re.sub('http\\S+\\s', ' ', txt)\n",
    "    cleanText = re.sub('RT|cc', ' ', cleanText)\n",
    "    cleanText = re.sub('#\\S+\\s', ' ', cleanText)\n",
    "    cleanText = re.sub('@\\S+', '  ', cleanText)  \n",
    "    cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
    "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText) \n",
    "    cleanText = re.sub('\\s+', ' ', cleanText)\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['resume'] = df['resume'].apply(lambda x: cleanResume(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(df['category'])\n",
    "df['category'] = le.transform(df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf.fit(df['resume'])\n",
    "requredTaxt  = tfidf.transform(df['resume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(requredTaxt, df['category'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6223067173637515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = OneVsRestClassifier(KNeighborsClassifier())\n",
    "clf.fit(X_train,y_train)\n",
    "ypred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# tfidf = TfidfVectorizer(stop_words='english')\n",
    "# pickle.dump(tfidfd,open('tfidf.pkl','wb'))\n",
    "# pickle.dump(clf, open('clf.pkl', 'wb'))\n",
    "\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer if not already imported\n",
    "from sklearn.svm import SVC  # Import the appropriate classifier if you're using SVC\n",
    "\n",
    "# Create and fit your TfidfVectorizer and classifier\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    " # Replace this with your specific classifier\n",
    "\n",
    "# Fit the TfidfVectorizer and classifier to your data\n",
    "# Example:\n",
    "tfidf.fit(df['resume'])\n",
    "# clf.fit(your_features, your_labels)\n",
    "\n",
    "# Save the TfidfVectorizer and classifier to pickle files\n",
    "with open('tfidf.pkl', 'wb') as tfidf_file:\n",
    "    pickle.dump(tfidf, tfidf_file)\n",
    "\n",
    "with open('clf.pkl', 'wb') as clf_file:\n",
    "    pickle.dump(clf, clf_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume_parser.py\n",
    "\n",
    "# import pdfplumber\n",
    "\n",
    "# def parse_resume(pdf_file):\n",
    "#     parsed_data = {\"email\": \"\", \"phone\": \"\", \"name\": \"\"}\n",
    "#     with pdfplumber.open(pdf_file) as pdf:\n",
    "#         text = \"\"\n",
    "#         for page in pdf.pages:\n",
    "#             text += page.extract_text()\n",
    "#         # Implement your logic to extract email, phone, and name here\n",
    "#         # Example:\n",
    "#         if \"Email:\" in text:\n",
    "#             parsed_data[\"email\"] = text.split(\"Email:\")[1].split()[0]\n",
    "#         if \"Phone:\" in text:\n",
    "#             parsed_data[\"phone\"] = text.split(\"Phone:\")[1].split()[0]\n",
    "#     return parsed_data\n",
    "\n",
    "\n",
    "import pickle, re, os\n",
    "def parse_resume(pdf_file):\n",
    "    pickle.dump(tfidfd,open('tfidf.pkl','wb'))\n",
    "    pickle.dump(clf, open('clf.pkl', 'wb'))\n",
    "    clf = pickle.load(clf_file)\n",
    "\n",
    "    def cleanResume(txt):\n",
    "        cleanText = re.sub('http\\S+\\s', ' ', txt)\n",
    "        cleanText = re.sub('RT|cc', ' ', cleanText)\n",
    "        cleanText = re.sub('#\\S+\\s', ' ', cleanText)\n",
    "        cleanText = re.sub('@\\S+', '  ', cleanText)  \n",
    "        cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
    "        cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText) \n",
    "        cleanText = re.sub('\\s+', ' ', cleanText)\n",
    "        return cleanText\n",
    "\n",
    "    # Clean the input resume\n",
    "    cleaned_resume = cleanResume(pdf_file)\n",
    "\n",
    "    # Transform the cleaned resume using the trained TfidfVectorizer\n",
    "    input_features = tfidfd.transform([cleaned_resume])\n",
    "\n",
    "    # Make the prediction using the loaded classifier\n",
    "    prediction_id = clf.predict(input_features)[0]\n",
    "\n",
    "    # Map category ID to category name\n",
    "    category_mapping = {\n",
    "        15: \"Java Developer\",\n",
    "        23: \"Testing\",\n",
    "        8: \"DevOps Engineer\",\n",
    "        20: \"Python Developer\",\n",
    "        24: \"Web Designing\",\n",
    "        12: \"HR\",\n",
    "        13: \"Hadoop\",\n",
    "        3: \"Blockchain\",\n",
    "        10: \"ETL Developer\",\n",
    "        18: \"Operations Manager\",\n",
    "        6: \"Data Science\",\n",
    "        22: \"Sales\",\n",
    "        16: \"Mechanical Engineer\",\n",
    "        1: \"Arts\",\n",
    "        7: \"Database\",\n",
    "        11: \"Electrical Engineering\",\n",
    "        14: \"Health and fitness\",\n",
    "        19: \"PMO\",\n",
    "        4: \"Business Analyst\",\n",
    "        9: \"DotNet Developer\",\n",
    "        2: \"Automation Testing\",\n",
    "        17: \"Network Security Engineer\",\n",
    "        21: \"SAP Developer\",\n",
    "        5: \"Civil Engineer\",\n",
    "        0: \"Advocate\",\n",
    "    }\n",
    "\n",
    "    category_name = category_mapping.get(prediction_id, \"Unknown\")\n",
    "    return category_name\n",
    "\n",
    "    # print(\"Predicted Category:\", category_name)\n",
    "    # print(prediction_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\at\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.10.4)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\at\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from PyPDF2) (4.6.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_resume========= AASMA AAS MUHAMMAD 0092304 4099 635 aasma aas muhammad www linkedin com in aasma aas muhammad 0a8726184 github com Aasma Aas SUMMARY Enthusiastic Data Scientist with a solid foundation in front end development having more than 2 years of experience who wants to transition into the data science world Experienced in various data science projects including machine learning deep learning and web scraping I am confident that my skills make me the best candidate for a data science role EXPERIENCE Arhamsoft Pvt Ltd FRONT END WEB DEVELOPER June 2021 June 2023 Developed fully responsive web designs primarily for blockchain projects ReactJs Vuejs while also contributing to projects on WordPress and Shopify platforms Worked closely with designers to ensure the correct implementation of UI components layouts and visuals Designed an animated website using GSAP library in reactJs or simple javascript both Developed a user friendly interface for the blockchain project Implemented various features and functionality using javascript reactjs and jQuery Tested the front end side of the project and fix bugs if any Collaborated with other developers to ensure the smooth functioning of the website Ensured that all coding standards are followed while developing the website Information Technolgy development ITD FRONT END WEB DEVELOPER Dec 2020 May 2021 Designed responsive websites using Html5 CSS3 Bootsrap5 and worked in wordpress projects Responsible for designing HTML and testing which includes browser tests throughout the project from start to end AkzoNobel PK Intern July 2017 Aug 2017 Closely work with the IT team to su essfully implement the Cisco Telepresence project Conducted in depth exploration and analysis of the Cisco Telepresence technology to gain a comprehensive understanding of its functionalities and capabilities Create detailed and informative manual documentation ensuring clear guidelines for the setup operation and maintenance of the Cisco Telepresence system TECHNICAL SKILLS Programming Python C Database SQL Postgres Web Technology Javascript Html ReactJs React Bootstrap GSAP Shopify Polaris CSS jQuery Tailwind CSS Libraries Pandas NumPy Scikit Learn Matplotlib Keras Tensorflow Data Science skills Exploratory Data Analysis Data Visualization Feature Engineering Machine Learning Natural Language Processing NLP Computer Vision Deep Learning Web Scraping Certifications Machine Learning with Python IBM Skills Network Building Deep Learning Models with TensorFlow Coursera Standford WIDS Datathon Workshop 2023 University of Management and Technology EDUCATION MS Computer Science March 2022 continue University of Management and Technology Pakistan 3 48 GPA Relevant coursework Advanced Algorithm Analysis Computer vision Natural Language Processing Advanced Artificial intelligence Master s Information and Communication Technology Sep 2016 Jan 2019 University of Management and Technology Pakistan Relevant coursework Database Object Oriented Programming Advanced Computer Networks Software Quality As surance Software Project Management Bachelor Double Mathematics and Physics March 2014 Aug 2016 University of Punjab Pakistan Relevant coursework Calculus with Analytic Geometry Mathematical Methods Calculus and Linear Algebra Physics Mechanics Waves and Oscillation Thermodynamics Electronics and Modern Physics ACADEMIC PROJECTS Thesis Suspicious Activity Detection UCF Crime images Dataset Spring 2023 Computer Vision Image Recognition Technology CNN LSTM CNN VGG16 Keras transfer learning model all type of DenseNet Resnet and VGG16 19 NumPy Python Flask OpenCV Keres TensorFlow A python based application that can detect crime images Robbery Shoplifting and Fighting using deep learning models and along with that research paper is also written FYP Journal Management System Spring 2018 Web Development Technology Java MySQL JavaScript Html CSS Bootstrap The Journal Management System JMS is a web based application developed using Java designed to streamline the process of article and research paper submission review and publication within the academic and research community Alzheimer s Disease Prediction using Machine Learning With XAI Fall 2022 Machine Learning Classification Technology Python NumPy Pandas Scikit learn matplotlib SVM AdaBoost Logistic Regression Random Forest Explainable AI Classification of Alzheimer s disease where I have performed proper EDA in well manner apply ML algorithm to classify disease using trained dataset and I have also applied the Explainable AI technique Lime which highlights the most important features Chatbot using Dialogue Flow Natural Language Processing Technology ReactJs Python Pandas Dialogue Api Build the Chatbot using Dialogue Api and integrated into a user friendly web page designed using ReactJs tech nology Web Scraping Technology Beautiful Soap Python Pandas Scaping of user views of thousands of products ACTIVITIES VOLUNTEER of Local Government 2023 present Member of Management Committee\n"
     ]
    }
   ],
   "source": [
    "def pdfextract(pdf_file):\n",
    "    fileReader = PyPDF2.PdfReader(open(pdf_file,'rb'))\n",
    "    countpage = len(fileReader.pages)\n",
    "    count = 0\n",
    "    txt = []\n",
    "    while count < countpage:\n",
    "        pageObj = fileReader.pages[count]\n",
    "        count +=1\n",
    "        t = pageObj.extract_text()\n",
    "        txt.append(t)\n",
    "    return txt\n",
    "\n",
    "pdf_file = './Aasma_Aas_Muhammad___DataScience.pdf'\n",
    "pdfextract(pdf_file)\n",
    "\n",
    "pdf_text = pdfextract(pdf_file)\n",
    "pdf_text = \" \".join(pdf_text)\n",
    "\n",
    "def cleanResume(txt):\n",
    "    cleanText = re.sub('http\\S+\\s', ' ', txt)\n",
    "    cleanText = re.sub('RT|cc', ' ', cleanText)\n",
    "    cleanText = re.sub('#\\S+\\s', ' ', cleanText)\n",
    "    cleanText = re.sub('@\\S+', '  ', cleanText)  \n",
    "    cleanText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', cleanText)\n",
    "    cleanText = re.sub(r'[^\\x00-\\x7f]', ' ', cleanText) \n",
    "    cleanText = re.sub('\\s+', ' ', cleanText)\n",
    "    return cleanText\n",
    "\n",
    "# Clean the input resume\n",
    "cleaned_resume = cleanResume(pdf_text)\n",
    "print('cleaned_resume=========', cleaned_resume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\FlaskWebAPP1\\app\\authentication\\Parser.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/FlaskWebAPP1/app/authentication/Parser.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer(stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/FlaskWebAPP1/app/authentication/Parser.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Transform the cleaned resume using the trained TfidfVectorizer\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/FlaskWebAPP1/app/authentication/Parser.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_features \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39;49mtransform([cleaned_resume])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/FlaskWebAPP1/app/authentication/Parser.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Make the prediction using the loaded classifier\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/FlaskWebAPP1/app/authentication/Parser.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m prediction_id \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(input_features)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\At\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2101\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2085\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, raw_documents):\n\u001b[0;32m   2086\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2087\u001b[0m \n\u001b[0;32m   2088\u001b[0m \u001b[39m    Uses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2099\u001b[0m \u001b[39m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2101\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m, msg\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mThe TF-IDF vectorizer is not fitted\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   2103\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mtransform(raw_documents)\n\u001b[0;32m   2104\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mtransform(X, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\At\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:1345\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     fitted \u001b[39m=\u001b[39m [\n\u001b[0;32m   1341\u001b[0m         v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mvars\u001b[39m(estimator) \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m v\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1342\u001b[0m     ]\n\u001b[0;32m   1344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fitted:\n\u001b[1;32m-> 1345\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "# Transform the cleaned resume using the trained TfidfVectorizer\n",
    "input_features = tfidf.transform([cleaned_resume])\n",
    "\n",
    "# Make the prediction using the loaded classifier\n",
    "prediction_id = clf.predict(input_features)[0]\n",
    "\n",
    "tfidf.fit(your_text_data)\n",
    "clf.fit(your_features, your_labels)\n",
    "\n",
    "# Map category ID to category name\n",
    "category_mapping = {\n",
    "        15: \"Java Developer\",\n",
    "        23: \"Testing\",\n",
    "        8: \"DevOps Engineer\",\n",
    "        20: \"Python Developer\",\n",
    "        24: \"Web Designing\",\n",
    "        12: \"HR\",\n",
    "        13: \"Hadoop\",\n",
    "        3: \"Blockchain\",\n",
    "        10: \"ETL Developer\",\n",
    "        18: \"Operations Manager\",\n",
    "        6: \"Data Science\",\n",
    "        22: \"Sales\",\n",
    "        16: \"Mechanical Engineer\",\n",
    "        1: \"Arts\",\n",
    "        7: \"Database\",\n",
    "        11: \"Electrical Engineering\",\n",
    "        14: \"Health and fitness\",\n",
    "        19: \"PMO\",\n",
    "        4: \"Business Analyst\",\n",
    "        9: \"DotNet Developer\",\n",
    "        2: \"Automation Testing\",\n",
    "        17: \"Network Security Engineer\",\n",
    "        21: \"SAP Developer\",\n",
    "        5: \"Civil Engineer\",\n",
    "        0: \"Advocate\",\n",
    "    }\n",
    "\n",
    "category_name = category_mapping.get(prediction_id, \"Unknown\")\n",
    "print('category_name=============',category_name)\n",
    "    # return category_name\n",
    "\n",
    "# Example usage:\n",
    "# category = parse_resume('sample_resume.pdf')\n",
    "# print(\"Predicted Category:\", category)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
